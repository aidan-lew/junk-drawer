{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4202140-140b-4349-95e3-bbe7bf27f014",
   "metadata": {},
   "source": [
    "# Stock scanner\n",
    "Search the news for stocks predicted to increase or decrease in value significantly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcce28c-c8aa-4b78-bcc1-514182162037",
   "metadata": {},
   "source": [
    "# First iteration\n",
    "Just the news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a57bd9-ef5f-4cdc-9318-c5ae1f26ee02",
   "metadata": {},
   "source": [
    "# Blackbox.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb19f4-83d6-495b-ac3a-505dead28053",
   "metadata": {},
   "source": [
    "## Prompt 1\n",
    "Promt: Using tools from (https://github.com/AI4Finance-Foundation) and any other useful tools (such as NLP models, DRL models, etc.) write a code that analyzes the news and identifies companies whos stacks are predicted to rise or fall significantly. Also, please walk me through each line of the code\n",
    "\n",
    "Maybe try for an entire index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803b7dbb-2a8f-4c11-87f2-ff62e7856292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidanlew/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "print('setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f136f019-cb63-4263-a302-c7bc0923fff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aidanlew/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aidanlew/nltk_data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(words)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Assume we have a pandas DataFrame `df` with a column `text` containing news articles\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#processes scraped data to make it suitable for ML\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words and lemmatize the words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "    # Join the words back together into a single string\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Assume we have a pandas DataFrame `df` with a column `text` containing news articles\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "589bbedc-b741-4ee8-aaba-5b3aa59d2a8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#make processed data numerical\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#make processed data numerical\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cd1d1c-4ff6-4b7b-a91a-28bb1af716e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#what we want to predict - rise or fall of stock\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m MultinomialNB()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#what we want to predict - rise or fall of stock\n",
    "y = df['target']\n",
    "\n",
    "#train model\n",
    "model = MultinomialNB()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af8e69e-c69d-4ebe-a3e0-70510218a99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|█████████████████| 48.0/48.0 [00:00<00:00, 33.1kB/s]\n",
      "config.json: 100%|█████████████████████████████| 570/570 [00:00<00:00, 2.51MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 2.52MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 3.37MB/s]\n",
      "model.safetensors: 100%|██████████████████████| 440M/440M [12:30<00:00, 587kB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_representation\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Preprocess the text using BERT\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text_bert)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "def preprocess_text_bert(text):\n",
    "    # Tokenize the text using the BERT tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Pass the inputs through the BERT model and get the final hidden state\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Take the first token's hidden state as the representation of the entire text\n",
    "    text_representation = last_hidden_states[:, 0, :]\n",
    "\n",
    "    return text_representation\n",
    "\n",
    "# Preprocess the text using BERT\n",
    "df['text'] = df['text'].apply(preprocess_text_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f610db4-983e-4989-9efa-d84cb4ab5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text'].to_numpy()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592cc01-7959-4bf1-a896-88f50096cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Preprocess the text using BERT\n",
    "df['text'] = df['text'].apply(preprocess_text_bert)\n",
    "\n",
    "# Convert the text into numerical features\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# Train the machine learning model on the features and target variable\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058789e2-fd52-4e38-95b5-f6c7c7957578",
   "metadata": {},
   "source": [
    "## Prompt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae72ae2-a8e7-4cec-a5d0-3b78cb6c586f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Promt: Using FinGPT,(https://github.com/AI4Finance-Foundation/FinGPT) & (https://huggingface.co/FinGPT), write a code that trains FinGPT using this dataset: (https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) to analyze the news and identify companies whos stacks are predicted to rise or fall significantly. Use any other nessesary tools and please walk me through each line of the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d33f3ec-0f9f-425f-a5f2-2c394a9a4538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset\n",
    "print ('setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fff2d72-a623-4df6-8da5-e7f5f66a11e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aidanlew/Desktop/cmb/codes'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62f7b4a-1055-4997-95d2-3a5aa9295b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aidanlew/Desktop/cmb/codes/AI4Finance\n"
     ]
    }
   ],
   "source": [
    "cd /Users/aidanlew/Desktop/cmb/codes/AI4Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e833bb2-2650-4606-9d5d-4c045966c3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "FinGPT does not appear to have a file named config.json. Checkout 'https://huggingface.co/FinGPT/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinGPT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinGPT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:773\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 773\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    776\u001b[0m     config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1100\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1098\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1100\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1102\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/transformers/configuration_utils.py:634\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    636\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/transformers/utils/hub.py:356\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 356\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         )\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: FinGPT does not appear to have a file named config.json. Checkout 'https://huggingface.co/FinGPT/None' for available files."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"FinGPT\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"FinGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd4b4556-0a83-4db8-802a-7f0509d657da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to resolve any data file that matches '['train[-._ 0-9/]**', '**[-._ 0-9/]train[-._ 0-9/]**', 'training[-._ 0-9/]**', '**[-._ 0-9/]training[-._ 0-9/]**']' at /Users/aidanlew/Desktop/cmb/codes/AI4Finance/FinGPT with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'BLP', 'BMP', 'DIB', 'BUFR', 'CUR', 'PCX', 'DCX', 'DDS', 'PS', 'EPS', 'FIT', 'FITS', 'FLI', 'FLC', 'FTC', 'FTU', 'GBR', 'GIF', 'GRIB', 'H5', 'HDF', 'PNG', 'APNG', 'JP2', 'J2K', 'JPC', 'JPF', 'JPX', 'J2C', 'ICNS', 'ICO', 'IM', 'IIM', 'TIF', 'TIFF', 'JFIF', 'JPE', 'JPG', 'JPEG', 'MPG', 'MPEG', 'MSP', 'PCD', 'PXR', 'PBM', 'PGM', 'PPM', 'PNM', 'PSD', 'BW', 'RGB', 'RGBA', 'SGI', 'RAS', 'TGA', 'ICB', 'VDA', 'VST', 'WEBP', 'WMF', 'EMF', 'XBM', 'XPM', 'aiff', 'au', 'avr', 'caf', 'flac', 'htk', 'svx', 'mat4', 'mat5', 'mpc2k', 'ogg', 'paf', 'pvf', 'raw', 'rf64', 'sd2', 'sds', 'ircam', 'voc', 'w64', 'wav', 'nist', 'wavex', 'wve', 'xi', 'mp3', 'opus', 'AIFF', 'AU', 'AVR', 'CAF', 'FLAC', 'HTK', 'SVX', 'MAT4', 'MAT5', 'MPC2K', 'OGG', 'PAF', 'PVF', 'RAW', 'RF64', 'SD2', 'SDS', 'IRCAM', 'VOC', 'W64', 'WAV', 'NIST', 'WAVEX', 'WVE', 'XI', 'MP3', 'OPUS', 'zip']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFinGPT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfingpt-sentiment-train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1769\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1770\u001b[0m )\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1773\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1501\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1502\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/load.py:1153\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LocalDatasetModuleFactoryWithScript(\n\u001b[1;32m   1148\u001b[0m         combined_path, download_mode\u001b[38;5;241m=\u001b[39mdownload_mode, dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path\n\u001b[1;32m   1149\u001b[0m     )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[0;32m-> 1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;66;03m# Try remotely\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_relative_path(path) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/load.py:632\u001b[0m, in \u001b[0;36mLocalDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m base_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\n\u001b[1;32m    629\u001b[0m patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    630\u001b[0m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_data_patterns_locally(base_path)\n\u001b[1;32m    631\u001b[0m )\n\u001b[0;32m--> 632\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mDataFilesDict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_local_or_remote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_ALLOWED_EXTENSIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m split_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    638\u001b[0m     split: infer_module_for_data_files(data_files_list) \u001b[38;5;28;01mfor\u001b[39;00m split, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    639\u001b[0m }\n\u001b[1;32m    640\u001b[0m module_name, builder_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(split_modules\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/data_files.py:783\u001b[0m, in \u001b[0;36mDataFilesDict.from_local_or_remote\u001b[0;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[1;32m    780\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m()\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, patterns_for_key \u001b[38;5;129;01min\u001b[39;00m patterns\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    782\u001b[0m     out[key] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 783\u001b[0m         \u001b[43mDataFilesList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_local_or_remote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatterns_for_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patterns_for_key, DataFilesList)\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m patterns_for_key\n\u001b[1;32m    791\u001b[0m     )\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/data_files.py:751\u001b[0m, in \u001b[0;36mDataFilesList.from_local_or_remote\u001b[0;34m(cls, patterns, base_path, allowed_extensions, use_auth_token)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_local_or_remote\u001b[39m(\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    748\u001b[0m     use_auth_token: Optional[Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    749\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFilesList\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    750\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m base_path \u001b[38;5;28;01mif\u001b[39;00m base_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(Path()\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 751\u001b[0m     data_files \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_patterns_locally_or_by_urls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_extensions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     origin_metadata \u001b[38;5;241m=\u001b[39m _get_origin_metadata_locally_or_by_urls(data_files, use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(data_files, origin_metadata)\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/data_files.py:356\u001b[0m, in \u001b[0;36mresolve_patterns_locally_or_by_urls\u001b[0;34m(base_path, patterns, allowed_extensions)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with any supported extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(allowed_extensions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(error_msg)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_files\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Unable to resolve any data file that matches '['train[-._ 0-9/]**', '**[-._ 0-9/]train[-._ 0-9/]**', 'training[-._ 0-9/]**', '**[-._ 0-9/]training[-._ 0-9/]**']' at /Users/aidanlew/Desktop/cmb/codes/AI4Finance/FinGPT with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'BLP', 'BMP', 'DIB', 'BUFR', 'CUR', 'PCX', 'DCX', 'DDS', 'PS', 'EPS', 'FIT', 'FITS', 'FLI', 'FLC', 'FTC', 'FTU', 'GBR', 'GIF', 'GRIB', 'H5', 'HDF', 'PNG', 'APNG', 'JP2', 'J2K', 'JPC', 'JPF', 'JPX', 'J2C', 'ICNS', 'ICO', 'IM', 'IIM', 'TIF', 'TIFF', 'JFIF', 'JPE', 'JPG', 'JPEG', 'MPG', 'MPEG', 'MSP', 'PCD', 'PXR', 'PBM', 'PGM', 'PPM', 'PNM', 'PSD', 'BW', 'RGB', 'RGBA', 'SGI', 'RAS', 'TGA', 'ICB', 'VDA', 'VST', 'WEBP', 'WMF', 'EMF', 'XBM', 'XPM', 'aiff', 'au', 'avr', 'caf', 'flac', 'htk', 'svx', 'mat4', 'mat5', 'mpc2k', 'ogg', 'paf', 'pvf', 'raw', 'rf64', 'sd2', 'sds', 'ircam', 'voc', 'w64', 'wav', 'nist', 'wavex', 'wve', 'xi', 'mp3', 'opus', 'AIFF', 'AU', 'AVR', 'CAF', 'FLAC', 'HTK', 'SVX', 'MAT4', 'MAT5', 'MPC2K', 'OGG', 'PAF', 'PVF', 'RAW', 'RF64', 'SD2', 'SDS', 'IRCAM', 'VOC', 'W64', 'WAV', 'NIST', 'WAVEX', 'WVE', 'XI', 'MP3', 'OPUS', 'zip']"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"FinGPT\", \"fingpt-sentiment-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55898b9-7598-4341-9ed8-ffac2e929f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text data using FinGPT \n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text using the FinGPT tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Pass the inputs through the FinGPT model and get the final hidden state\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Take the first token's hidden state as the representation of the entire text\n",
    "    text_representation = last_hidden_states[:, 0, :]\n",
    "\n",
    "    return text_representation\n",
    "\n",
    "# Preprocess the text data\n",
    "dataset['text_representation'] = dataset['text'].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63df515-59b2-4e41-a864-aeb43339abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X = np.stack(dataset['text_representation'])\n",
    "y = dataset['label'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0dd0e8-585c-4be5-8075-2c27e84ede5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for inputs, labels in zip(X_train, y_train):\n",
    "        inputs = torch.unsqueeze(inputs, dim=0)\n",
    "        labels = torch.unsqueeze(labels, dim=0)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs.logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf52ac-869a-46ad-9457-e94cfe0a193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "for inputs in X_test:\n",
    "    inputs = torch.unsqueeze(inputs, dim=0)\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs.logits, dim=1)\n",
    "    predictions.append(predicted.item())\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f8144-0831-4a61-bd00-815954ca3c24",
   "metadata": {},
   "source": [
    "## Prompt 3\n",
    "Using the AI4Finance agent:\n",
    "    Write me code in python using FinGPT that analyzes news from the web, gives a sentiment rating of the news, and then makes a prediction about the movment of a stockprice based on that news analysis. Make sure that the code includes the download of all dependencies and is able to run on google colab\n",
    "    \n",
    "was able to massage it with chatgpt to work!\n",
    "Run on colab\n",
    "Replace the sklearn model with finrl, something I want\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e6aa03-8875-4eae-8502-fac79947d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install git+https://github.com/huggingface/accelerate.git\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b408e75-34d1-4ff6-a246-ad2c1e8e3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6557b-f8e5-4087-95ba-0ea6577dac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5df42d-ef61-471e-8149-a5c6dc658b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizerFast\n",
    "from peft import PeftModel  # 0.5.0\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c243f2f-7158-45f2-916f-774acad0b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#have to have access to LLama -  via hugging face\n",
    "### THIS IS ABlE TO RUN ON a T4!!!\n",
    "# https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster\n",
    "# https://github.com/AI4Finance-Foundation/FinGPT/blob/master/FinGPT_Inference_Llama2_13B_falcon_7B_for_Beginners.ipynb\n",
    "### see how the flow works for these, look at the hugging face page dataset for training.\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,   # optional if you have enough VRAM\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, 'FinGPT/fingpt-forecaster_dow30_llama2-7b_lora')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be82ae-ce81-4380-a72f-827c9312a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_news_articles(ticker, num_articles):\n",
    "    url = f\"https://www.google.com/search?q={ticker}+news\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = []\n",
    "    for result in soup.find_all('div', class_='g'):\n",
    "        article = {}\n",
    "        article['title'] = result.find('h3').text\n",
    "        article['link'] = result.find('a')['href']\n",
    "        article['text'] = result.find('span', class_='st').text\n",
    "        articles.append(article)\n",
    "        if len(articles) == num_articles:\n",
    "            break\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8653159a-0446-4a34-b5cb-993b4b1232cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_news_articles(ticker, num_articles):\n",
    "    url = f\"https://www.google.com/search?q={ticker}+news\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = []\n",
    "    for result in soup.find_all('div', class_='g'):\n",
    "        article = {}\n",
    "        article['title'] = result.find('h3').text\n",
    "        article['link'] = result.find('a')['href']\n",
    "        article['text'] = result.find('span', class_='st').text\n",
    "        articles.append(article)\n",
    "        if len(articles) == num_articles:\n",
    "            break\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72798c09-9878-4d49-bc06-9b3719d73f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_news_sentiment(articles):\n",
    "    sentiments = []\n",
    "    for article in articles:\n",
    "        inputs = tokenizer(article['text'], return_tensors='pt', max_length=512, truncation=True)\n",
    "        outputs = model(inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        sentiment = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "        sentiments.append(sentiment.detach().numpy())\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be06a51-9e64-440c-bac0-bb9198f16049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "def make_stock_price_prediction(ticker, sentiments):\n",
    "    stock_data = yf.download(ticker, period='1d', interval='1m')\n",
    "    \n",
    "    # Check if sentiments is a list of lists/tuples or a 2D numpy array\n",
    "    sentiments = np.array(sentiments)\n",
    "    \n",
    "    if sentiments.ndim == 1:  # Single list, can't access [1] element\n",
    "        sentiment_mean = np.mean(sentiments)\n",
    "    else:\n",
    "        sentiment_mean = np.mean(sentiments, axis=0)[1]  # Use positive sentiment score\n",
    "\n",
    "    print(f\"Sentiment mean: {sentiment_mean}\")\n",
    "    \n",
    "    stock_data['sentiment'] = sentiment_mean\n",
    "    X = stock_data[['sentiment']]\n",
    "    y = stock_data['Close']\n",
    "    \n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    \n",
    "    # Fit a HistGradientBoostingRegressor model\n",
    "    model = HistGradientBoostingRegressor()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction_input = np.array([[sentiment_mean]])\n",
    "    print(f\"Prediction input shape: {prediction_input.shape}\")\n",
    "    \n",
    "    prediction = model.predict(prediction_input)\n",
    "    \n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    \n",
    "    return prediction[0]  # Return the scalar prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917111f-a1c6-4ef3-b146-905197b47ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'  # replace with your desired ticker symbol\n",
    "num_articles = 10\n",
    "\n",
    "articles = scrape_news_articles(ticker, num_articles)\n",
    "sentiments = analyze_news_sentiment(articles)\n",
    "prediction = make_stock_price_prediction(ticker, sentiments)\n",
    "\n",
    "print(f\"Predicted stock price for {ticker}: {prediction}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d8f2d70-1cf2-49ee-863b-c4a4b6bf5c2e",
   "metadata": {},
   "source": [
    "[*********************100%%**********************]  1 of 1 completedSentiment mean: nan\n",
    "X shape: (361, 1)\n",
    "y shape: (361,)\n",
    "Prediction input shape: (1, 1)\n",
    "Prediction: [210.62308847]\n",
    "Predicted stock price for AAPL: 210.62308846682393"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59d8b1-da1c-420b-a087-6f9ff3a895d2",
   "metadata": {},
   "source": [
    "## Prompt 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e4b0fa-3235-4742-a643-d520502971ee",
   "metadata": {},
   "source": [
    "Using AI4Finance agent: Write me python code that can be run on google colab that scrapes news headlines every 10 minutes, gives a sentiment analysis of the headlines, and sends an alert if a headline is significantly positive or negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b250586-2bde-4c11-baf4-349a9ddc349c",
   "metadata": {},
   "source": [
    "- Can I just tell an LLM to do what I want it to do? I.e. just use it like an assistant that constantly scans without having to hard code it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc12a524-23f7-4134-b0f2-13aa08605585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/aidanlew/miniconda3/lib/python3.10/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in /Users/aidanlew/miniconda3/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: joblib in /Users/aidanlew/miniconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/aidanlew/miniconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.64.1)\n",
      "Requirement already satisfied: click in /Users/aidanlew/miniconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/aidanlew/miniconda3/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: schedule in /Users/aidanlew/miniconda3/lib/python3.10/site-packages (1.2.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement time (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for time\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement smtplib (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for smtplib\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install schedule\n",
    "!pip install time\n",
    "!pip install smtplib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abdb25a-6f42-4b8b-9e14-b51247e36d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import schedule\n",
    "import time\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dde124-de42-4604-a50e-25f42ed1d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up email alert settings\n",
    "SMTP_SERVER = \"smtp.gmail.com\"\n",
    "SMTP_PORT = 587\n",
    "FROM_EMAIL = \"jakumail14@gmail.com\"\n",
    "PASSWORD = \"$spam4ever$\"\n",
    "TO_EMAIL = \"aidanlew64@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c69813c-32c1-4888-bdd4-01bf8ebcee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up news source and sentiment analysis threshold\n",
    "#NEWS_SOURCE = \"https://www.reuters.com/business\"\n",
    "POSITIVE_THRESHOLD = 0.1\n",
    "NEGATIVE_THRESHOLD = -0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8bfb0d2-5c36-44ed-90f0-25f8159866df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GameStop Stock Soars Again After New Roaring K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stock market today: US indexes rally after coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stock market today: US stocks edge lower as AI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stock market today: US stocks jump to records ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The year of the election just delivered its fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  GameStop Stock Soars Again After New Roaring K...\n",
       "1  Stock market today: US indexes rally after coo...\n",
       "2  Stock market today: US stocks edge lower as AI...\n",
       "3  Stock market today: US stocks jump to records ...\n",
       "4  The year of the election just delivered its fi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_news(api_key, query='stock market', page_size=10):\n",
    "    url = f'https://newsapi.org/v2/everything?q={query}&pageSize={page_size}&apiKey={api_key}'\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    articles = response.json().get('articles', [])\n",
    "    return articles\n",
    "\n",
    "def process_articles(articles):\n",
    "    data = []\n",
    "    for article in articles:\n",
    "        title = article.get('title', '')\n",
    "        data.append({'title': title})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "news_api_key = 'd6bf0775f7554f32ba23b76db2fa99f3'\n",
    "news_articles = fetch_news(news_api_key)\n",
    "headlines = process_articles(news_articles)\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845366ba-a29c-44e9-83f2-145c75b478ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13636363636363635, 0.35, 0.0, 0.0, 0.125, 0.16, 0.0, 0.0, -0.25, 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment_analysis(headlines):\n",
    "    sentiments = []\n",
    "    for headline in headlines['title']:  # Iterate over the 'title' column\n",
    "        blob = TextBlob(headline)\n",
    "        sentiments.append(blob.sentiment.polarity)\n",
    "    #headlines['sentiment'] = sentiments  # Add the sentiments to the DataFrame\n",
    "    return sentiments\n",
    "\n",
    "news_api_key = 'd6bf0775f7554f32ba23b76db2fa99f3'\n",
    "news_articles = fetch_news(news_api_key)\n",
    "processed_data = process_articles(news_articles)\n",
    "sentiments = sentiment_analysis(processed_data)\n",
    "sentiments\n",
    "#headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735c2981-276e-4c9b-b137-e2bad14c46fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_alert(headline, sentiment):\n",
    "    msg = MIMEText(f\"Alert: {headline} - Sentiment: {sentiment:.2f}\")\n",
    "    msg['Subject'] = \"News Alert\"\n",
    "    msg['From'] = FROM_EMAIL\n",
    "    msg['To'] = TO_EMAIL\n",
    "    server = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)\n",
    "    server.starttls()\n",
    "    server.login(FROM_EMAIL, PASSWORD)\n",
    "    server.sendmail(FROM_EMAIL, TO_EMAIL, msg.as_string())\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63619a66-0b8f-490c-8ab1-36b7d5376964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    news_api_key = 'd6bf0775f7554f32ba23b76db2fa99f3'\n",
    "    news_articles = fetch_news(news_api_key)\n",
    "    processed_data = process_articles(news_articles)\n",
    "    sentiments = sentiment_analysis(processed_data)\n",
    "    for headline, sentiment in zip(headlines, sentiments):\n",
    "        if sentiment > POSITIVE_THRESHOLD or sentiment < NEGATIVE_THRESHOLD:\n",
    "            send_alert(headline, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1e5eae-3801-458a-942d-80dbf19c97a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SMTPAuthenticationError",
     "evalue": "(535, b'5.7.8 Username and Password not accepted. For more information, go to\\n5.7.8  https://support.google.com/mail/?p=BadCredentials 00721157ae682-63f12699541sm39245177b3.66 - gsmtp')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSMTPAuthenticationError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m schedule\u001b[38;5;241m.\u001b[39mevery(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mminutes\u001b[38;5;241m.\u001b[39mdo(job)  \u001b[38;5;66;03m# Run every 1 minutes\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mschedule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 1 second delay\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/schedule/__init__.py:854\u001b[0m, in \u001b[0;36mrun_pending\u001b[0;34m()\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pending\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls :meth:`run_pending <Scheduler.run_pending>` on the\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m    :data:`default scheduler instance <default_scheduler>`.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 854\u001b[0m     \u001b[43mdefault_scheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pending\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/schedule/__init__.py:101\u001b[0m, in \u001b[0;36mScheduler.run_pending\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m runnable_jobs \u001b[38;5;241m=\u001b[39m (job \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs \u001b[38;5;28;01mif\u001b[39;00m job\u001b[38;5;241m.\u001b[39mshould_run)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m job \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(runnable_jobs):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/schedule/__init__.py:173\u001b[0m, in \u001b[0;36mScheduler._run_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, CancelJob) \u001b[38;5;129;01mor\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m CancelJob:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_job(job)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/schedule/__init__.py:691\u001b[0m, in \u001b[0;36mJob.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CancelJob\n\u001b[1;32m    690\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning job \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 691\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_run \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_next_run()\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mjob\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m headline, sentiment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(headlines, sentiments):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentiment \u001b[38;5;241m>\u001b[39m POSITIVE_THRESHOLD \u001b[38;5;129;01mor\u001b[39;00m sentiment \u001b[38;5;241m<\u001b[39m NEGATIVE_THRESHOLD:\n\u001b[0;32m----> 8\u001b[0m         \u001b[43msend_alert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheadline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36msend_alert\u001b[0;34m(headline, sentiment)\u001b[0m\n\u001b[1;32m      6\u001b[0m server \u001b[38;5;241m=\u001b[39m smtplib\u001b[38;5;241m.\u001b[39mSMTP(SMTP_SERVER, SMTP_PORT)\n\u001b[1;32m      7\u001b[0m server\u001b[38;5;241m.\u001b[39mstarttls()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFROM_EMAIL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPASSWORD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m server\u001b[38;5;241m.\u001b[39msendmail(FROM_EMAIL, TO_EMAIL, msg\u001b[38;5;241m.\u001b[39mas_string())\n\u001b[1;32m     10\u001b[0m server\u001b[38;5;241m.\u001b[39mquit()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/smtplib.py:750\u001b[0m, in \u001b[0;36mSMTP.login\u001b[0;34m(self, user, password, initial_response_ok)\u001b[0m\n\u001b[1;32m    747\u001b[0m         last_exception \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# We could not login successfully.  Return result of last attempt.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m last_exception\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/smtplib.py:739\u001b[0m, in \u001b[0;36mSMTP.login\u001b[0;34m(self, user, password, initial_response_ok)\u001b[0m\n\u001b[1;32m    737\u001b[0m method_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauth_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m authmethod\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 739\u001b[0m     (code, resp) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauthmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_response_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_response_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# 235 == 'Authentication successful'\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# 503 == 'Error: already authenticated'\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m235\u001b[39m, \u001b[38;5;241m503\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/smtplib.py:662\u001b[0m, in \u001b[0;36mSMTP.auth\u001b[0;34m(self, mechanism, authobject, initial_response_ok)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m235\u001b[39m, \u001b[38;5;241m503\u001b[39m):\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (code, resp)\n\u001b[0;32m--> 662\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m SMTPAuthenticationError(code, resp)\n",
      "\u001b[0;31mSMTPAuthenticationError\u001b[0m: (535, b'5.7.8 Username and Password not accepted. For more information, go to\\n5.7.8  https://support.google.com/mail/?p=BadCredentials 00721157ae682-63f12699541sm39245177b3.66 - gsmtp')"
     ]
    }
   ],
   "source": [
    "schedule.every(1).minutes.do(job)  # Run every 1 minutes\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1) # 1 second delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada9d66a-0ff2-4b5e-b923-bbfa5824666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6c848-4294-4f65-92dc-43595b75c7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88b9ec88-3ec9-4904-97e0-b356d99ebbeb",
   "metadata": {},
   "source": [
    "# From AI4financeGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea6f13e-fa7d-4250-92d3-db7585489ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "jsonl_path = \"../data/dataset_new.jsonl\"\n",
    "save_path = '../data/dataset_new'\n",
    "\n",
    "\n",
    "if os.path.exists(jsonl_path):\n",
    "    os.remove(jsonl_path)\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "\n",
    "directory = \"../data\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9593ad3-d261-424d-b543-a1eeffd2071e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec==2023.9.2 in ./monkey_env/lib/python3.10/site-packages (2023.9.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install fsspec==2023.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf2e13d-2874-4c1b-b6a8-e75f1fea1678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/Users/aidanlew/.cache/huggingface/datasets/zeroshot___csv/zeroshot--twitter-financial-news-sentiment-a86f08210b79b812/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 57.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction'],\n",
       "    num_rows: 9543\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "dic = {\n",
    "    0:\"negative\",\n",
    "    1:'positive',\n",
    "    2:'neutral',\n",
    "}\n",
    "\n",
    "#try with the fin-gpt-sentiment analysis dataset\n",
    "tfns = load_dataset('zeroshot/twitter-financial-news-sentiment')\n",
    "tfns = tfns['train']\n",
    "tfns = tfns.to_pandas()\n",
    "tfns['label'] = tfns['label'].apply(lambda x:dic[x])\n",
    "tfns['instruction'] = 'What is the sentiment of this tweet? Please choose an answer from {negative/neutral/positive}.'\n",
    "tfns.columns = ['input', 'output', 'instruction']\n",
    "tfns = datasets.Dataset.from_pandas(tfns)\n",
    "tfns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74444573-c3bb-409c-ad16-eda873557cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidanlew/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/table.py:1381: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/Users/aidanlew/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/datasets/table.py:1407: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19086, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dataset = datasets.concatenate_datasets([tfns]*2)\n",
    "train_dataset = tmp_dataset\n",
    "print(tmp_dataset.num_rows)\n",
    "\n",
    "all_dataset = train_dataset.shuffle(seed = 42)\n",
    "all_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f55a1e-fbe1-4ebe-b88b-cf3c8566a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7217b53e-4772-4084-9f4e-3bf1316d6447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_example(example: dict) -> dict:\n",
    "    context = f\"Instruction: {example['instruction']}\\n\"\n",
    "    if example.get(\"input\"):\n",
    "        context += f\"Input: {example['input']}\\n\"\n",
    "    context += \"Answer: \"\n",
    "    target = example[\"output\"]\n",
    "    return {\"context\": context, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7678b698-d6e2-48a2-b40b-0c6e817e6848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_list = []\n",
    "for item in all_dataset.to_pandas().itertuples():\n",
    "    tmp = {}\n",
    "    tmp[\"instruction\"] = item.instruction\n",
    "    tmp[\"input\"] = item.input\n",
    "    tmp[\"output\"] = item.output\n",
    "    data_list.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4b5e8-f6f4-4b71-ae24-b63fd30b361a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# for error on below code\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "https://stackoverflow.com/questions/67998191/importerror-iprogress-not-found-please-update-jupyter-and-ipywidgets-although"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713b7347-d634-474c-b243-e36c0504e1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save to a jsonl file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/dataset_new.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformatting..\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      4\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(format_example(example)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/tqdm/notebook.py:238\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    237\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/cmb/codes/monkey_env/lib/python3.10/site-packages/tqdm/notebook.py:113\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    115\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# save to a jsonl file\n",
    "with open(\"../data/dataset_new.jsonl\", 'w') as f:\n",
    "    for example in tqdm(data_list, desc=\"formatting..\"):\n",
    "        f.write(json.dumps(format_example(example)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96ae57c-cb7d-4708-8524-866031635910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in ./monkey_env/lib/python3.10/site-packages (8.1.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./monkey_env/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./monkey_env/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./monkey_env/lib/python3.10/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in ./monkey_env/lib/python3.10/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in ./monkey_env/lib/python3.10/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: decorator in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./monkey_env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./monkey_env/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./monkey_env/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./monkey_env/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in ./monkey_env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in ./monkey_env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in ./monkey_env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in ./monkey_env/lib/python3.10/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: widgetsnbextension in ./monkey_env/lib/python3.10/site-packages (4.0.10)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/bin/jupyter-nbextension\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/jupyter_core/application.py\", line 277, in launch_instance\n",
      "    return super().launch_instance(argv=argv, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/notebook/nbextensions.py\", line 972, in start\n",
      "    super().start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/jupyter_core/application.py\", line 266, in start\n",
      "    self.subapp.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/notebook/nbextensions.py\", line 882, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/notebook/nbextensions.py\", line 855, in toggle_nbextension_python\n",
      "    return toggle(module,\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/notebook/nbextensions.py\", line 470, in enable_nbextension_python\n",
      "    return _set_nbextension_state_python(True, module, user, sys_prefix,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/notebook/nbextensions.py\", line 368, in _set_nbextension_state_python\n",
      "    m, nbexts = _get_nbextension_metadata(module)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/notebook/nbextensions.py\", line 1107, in _get_nbextension_metadata\n",
      "    m = import_item(module)\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/traitlets/utils/importstring.py\", line 38, in import_item\n",
      "    return __import__(parts[0])\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'widgetsnbextension'\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install widgetsnbextension\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254697ad-ea4e-4e64-97f0-ab3a853b464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Send an HTTP request to the website\n",
    "url = \"https://www.examplenewswebsite.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 3: Identify the HTML elements containing the desired information\n",
    "headline_elements = soup.find_all(\"h2\", class_=\"news-headline\")\n",
    "\n",
    "# Step 4: Extract and display the headlines\n",
    "print(\"Top News Headlines:\")\n",
    "for headline in headline_elements:\n",
    "    print(headline.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa86941-ee52-4175-9158-79b81aa85c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9834b2-83c3-4596-82a3-a132f0ad7d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
