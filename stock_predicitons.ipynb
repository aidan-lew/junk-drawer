{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843e5d55-bcbc-4d9a-aecc-c101b11615d2",
   "metadata": {},
   "source": [
    "# OG from Boris https://github.com/borisbanushev/stockpredictionai/blob/f1984b952b86d61653758ddcd0dc6dd3735a5e5b/TradingAI.ipynb\n",
    "\n",
    "\n",
    "\n",
    "# For data and other code - friend of Boris – https://github.com/manganganath/stock_price_trend_fft and https://github.com/Q4living \n",
    "\n",
    "Using the latest advancements in AI to predict stock market movements\n",
    "       In this noteboook I will create a complete process for predicting stock price movements. Follow along and we will achieve some pretty good results. For that purpose we will use a Generative Adversarial Network (GAN) with LSTM, a type of Recurrent Neural Network, as generator, and a Convolutional Neural Network, CNN, as a discriminator. We use LSTM for the obvious reason that we are trying to predict time series data. Why we use GAN and specifically CNN as a discriminator? That is a good question: there are special sections on that later.\n",
    "\n",
    "       We will go into greater details for each step, of course, but the most difficult part is the GAN: very tricky part of successfully training a GAN is getting the right set of hyperparameters. For that reason we will use Bayesian optimisation (along with Gaussian processes) and Reinforcement learning (RL) for deciding when and how to change the GAN's hyperparamenets (the exploration vs. exploitation dillema). In creating the reinforcement learning we will use the most recent advancements in the field, such as Rainbow and PPO.\n",
    "\n",
    "       We will use a lot of different types of input data. Along with the stock's historical trading data and technical indicators, we will use the newest advancements in NLP (using 'Bidirectional Embedding Representations from Transformers', BERT, sort of a transfer learning for NLP) to create sentiment analysis (as a source for fundamental analysis), Fourier transforms for extracting overall trend directions, Stacked autoencoders for identifying other high-level features, Eigen portfolios for finding correlated assets, autoregressive integrated moving average (ARIMA) for the stock function approximation, and many more, in order to capture as much information, patterns, dependencies, etc, as possible about the stock. As we all know, the more (data) the merrier. Predicting stock price movements is an extremely complex task, so the more we know about the stock (from different perspectives) the higher our changes are.\n",
    "\n",
    "       For the purpose of creating all neural nets we will use MXNet and it's high-level API - Gluon, and train them on multiple GPUs.\n",
    "\n",
    "Note: Altough I try to get into detials of the math and the mechanisms behind almost all algorithms and techniques, this notebook is not explicitly intended to explain how machine/deep learning, or the stock markets, work. The purpose is rather to show how we can use different techniques and algoritms for the purpose of accurately preducting stock price movements, and to also give rationale behind the reason and usefulness of using each technique at each step.\n",
    "\n",
    "Notebook created: January 9, 2019.\n",
    "\n",
    "Figure 1 - The overall architecture of our work\n",
    "\n",
    "Table of content\n",
    "Introduction\n",
    "Acknowledgement\n",
    "The data\n",
    "Correlated assets\n",
    "Tecnical indicators\n",
    "Fundamental analysis\n",
    "Bidirectional Embedding Representations from Transformers - BERT\n",
    "Fourier transforms for trend analysis\n",
    "ARIMA as a feature\n",
    "Statistical checks\n",
    "Heteroskedasticity, multicollinearity, serial correlation\n",
    "Feature Engineering\n",
    "Feature importance with XGBoost\n",
    "Extracting high-level features with Stacked Autoencoders\n",
    "Activation function - GELU (Gaussian Error)\n",
    "Eigen portfolio with PCA\n",
    "Deep Unsupervised Learning for anomaly detection in derivatives pricing\n",
    "Generative Adversarial Network - GAN\n",
    "Why GAN for stock market prediction?\n",
    "Metropolis-Hastings GAN and Wasserstein GAN\n",
    "The Generator - One layer RNN\n",
    "LSTM or GRU\n",
    "The LSTM architecture\n",
    "Learning rate scheduler\n",
    "How to prevent overfitting and the bias-variance trade-off\n",
    "Custom weights initializers and custom loss metric\n",
    "The Discriminator - 1D CNN\n",
    "Why CNN as a discriminator?\n",
    "The CNN architecture\n",
    "Hyperparameters\n",
    "Hyperparameters optimization\n",
    "Reinforcement learning for hyperparameters optimization\n",
    "Theory\n",
    "Rainbow\n",
    "PPO\n",
    "Further work on Reinforcement learning\n",
    "Bayesian optimization\n",
    "Gaussian process\n",
    "The result\n",
    "What is next?\n",
    "Disclaimer\n",
    "1. Introduction \n",
    "       Accurately predicting the stock markets is a complex task as there are millions of events and pre-conditions for a particilar stock to move in a particular direction. So we need to be able to capture as many of these pre-conditions are possible. We also need make several important assumptions: 1) markets are not 100% random, 2) history repeats, 3) markets follow people's rational behavior, and 4) the markets are 'perfect'. And, please, do read the Disclaimer at the bottom.\n",
    "\n",
    "       We will try to predict the price movements of Goldman Sachs (NYSE: GS). For the purpose, we will use daily closing price from January 1st, 2010 to December 31st, 2018 (seven years for training purposes and two years for validation purposes). We will use the terms 'Goldman Sachs' and 'GS' interchangeably.\n",
    "\n",
    "2. Acknowledgement \n",
    "       Before we continue, I'd like to thank my friends Nuwan and Thomas without whose ideas and support I wouldn't have been able to create this work.\n",
    "\n",
    "3. The Data \n",
    "       We need to understand what affects whether GS's stock price will move up or down. It is what people as a whole think. Hence, we need to incorporate as much information (depicting the stock from different aspects and angles) as possible. (We will use daily data - 1,585 days to train the various algorithms (70% of the data we have) and predict the next 680 days (test data). Then we will compare the predicted results with a test (hold-out) data. Each type of data (we will refer to it as feature) is explained in greater detail in later sections, but, as a high level overview, the features we will use are:\n",
    "\n",
    "Correlated assets - these are other assets (any type, not necessarily stocks, such as commodities, FX, indices, or even fixed income securities). A big company, such as Goldman Sachs, obviously doesn't 'live' in an isolated world - it depends on, and interacts with, many external factors, including its competitors, clients, the global economy, the geo-political situation, fiscal and monetary policies, access to capital, etc. The details are listed later.\n",
    "Technical indicators - a lot of investors follow technical indicators. We will include the most popular indicators as independent features. Among them - 7 and 21 days moving average, exponential moving average, momentum, Bollinger bands, MACD.\n",
    "Fundamental analysis - A very important feature indicating whether a stock might move up or down. There are two features that can be used in fundamental analysis: 1) Analysing the company performance using 10-K and 10-Q reports, analysing ROE and P/E, etc (we will not use this), and 2) News - potentially news can indicate upcoming events that can potentially move the stock in certain direction. We will read all daily news for Goldman Sachs and extract whether the total sentiment about Goldman Sachs on that day is positive, neutral, or negative (as a score from 0 to 1). As many investors closely read the news and make investment decisions based (partially of course) on news, there is a somewhat high chance that if, say, the news for Goldman Sachs today are extremely positive the stock will surge tomorrow. One crucial point, we will perform feature importance (meaning how indicative it is for the movement of GS) on absolutely every feature (including this one) later on and decide whether we will use it. More on that later.\n",
    "       For the purpose of creating accurate sentiment prediction we will use Neural Language Processing (NLP). We will use BERT - Google's recently announced NLP approach for transfer learning for sentiment classification stock news sentiment extraction.\n",
    "Fourier transforms - Along with the daily closing price, we will create Fourier transforms in order to generalize several long- and short-term trends. Using these transforms we will eliminate a lot of noise (random walks) and create approximations of the real stock movement. Having trend approximations can help the LSTM network pick it's prediction trends more accurately.\n",
    "Autoregressive Integrated Moving Average (ARIMA) - This was one of the most popular techniques for predicting future values of time series data (in the pre-neural networks ages). Let's add it and see if it comes off as an important predictive feature.\n",
    "Stacked autoencoders - most of the aforementioned features (fundamental analysis, technical analysis, etc) were found by people after decades of research. But maybe we have missed something. Maybe there are hidden correlations that people cannot comprehend due to the enormous amount of data points, events, assets, charts, etc. With stacked autoencoders (type of neural networks) we can use the power of computers and probably find new types of features that affect stock movements. Even though we will not be able to understand these features in human language, we will use them in the GAN.Deep Unsupervised learning for anomaly detection in options pricing. Explained later in section XXX.\n",
    "Next, having so many features, we need to perform a couple of important steps:\n",
    "\n",
    "Perform statistical checks for the 'quality' of the data. If the data we create if flawed, then no matter how sophisticated our algorithms are, the results will not be positive. The checks include making sure the data does not suffer from heteroskedasticity, multicollinearity, or serial correlation.\n",
    "Create feature importance. If a feature (e.g. another stock or a technical indicator) has no explanatory power to the stock we want to predict, then there no need for us to use it in the training of the neural nets. We will using XGBoost (eXtreme Gradient Boosting), a type of boosted tree regression algorithms.\n",
    "       As a final step of our data preparation, we will also create Eigen portfolios using Principal Component Analysis (PCA) in order to reduce the dimensionality of the features created from the autoencoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e7e821-4a8a-4016-a62e-ea323f4a6630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup complete\n"
     ]
    }
   ],
   "source": [
    "#Try with catboost\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet as mx\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3471ccd-e73a-49f5-9e71-96c895454e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca4430c-1961-4c86-9071-acc0de078f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = mx.cpu(); model_ctx=mx.cpu()\n",
    "mx.random.seed(1719)\n",
    "\n",
    "\n",
    "#Note: The purpose of this section (3. The Data) is to show the data preprocessing and to give rationale for using different sources of data, hence I will only use a subset of the full data (that is used for training).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8efcab25-6aca-40d8-9135-57a87216305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(x):\n",
    "    return datetime.datetime.strptime(x,'%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbcb7320-63e3-4afd-a003-244911efe2b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/panel_data_close.csv does not exist: 'data/panel_data_close.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a4b7f4baedd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_ex_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/panel_data_close.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/miniconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File data/panel_data_close.csv does not exist: 'data/panel_data_close.csv'"
     ]
    }
   ],
   "source": [
    "dataset_ex_df = pd.read_csv('data/panel_data_close.csv', header=0, parse_dates=[0], date_parser=parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054a1b1-d470-4707-aadb-4c1570e04488",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ex_df[['Date', 'GS']].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234b02c-1c51-49e6-b96f-6cdedcc51675",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} number of days in the dataset.'.format(dataset_ex_df.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7785e215-fe78-4d52-92d4-10ed87ab13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5), dpi=100)\n",
    "plt.plot(dataset_ex_df['Date'], dataset_ex_df['GS'], label='Goldman Sachs stock')\n",
    "plt.vlines(datetime.date(2016,4, 20), 0, 270, linestyles='--', colors='gray', label='Train/Test data cut-off')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Figure 2: Goldman Sachs stock price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5142153c-11d6-4dd0-9207-66be25f4a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_days = int(dataset_ex_df.shape[0]*.7)\n",
    "print('Number of training days: {}. Number of test days: {}.'.format(num_training_days, \\\n",
    "                                                                    dataset_ex_df.shape[0]-num_training_days))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc350cdf-6818-46ca-8086-cadfda62a14c",
   "metadata": {},
   "source": [
    "\n",
    "3.1. Correlated assets \n",
    "       As explained earlier we will use other assets as features, not only GS.\n",
    "\n",
    "       So what other assets would affect GS's stock movements? Good understanding of the company, its lines of businesses, competitive landscape, dependencies, suppliers and client type, etc is very important for picking the right set of correlated assets:\n",
    "\n",
    "First are the companies similar to GS. We will add JPMorgan Chace and Morgan Stanley, among others, to the dataset.\n",
    "As an investment bank, Goldman Sachs depends on the global economy. Bad or volatile economy means no M&As or IPOs, and possibly limited proprietary trading earnings. That is why we will include global economy indices. Also, we will include LIBOR (USD and GBP denominated) rate, as possibly shocks in the economy might be accounted for by analysts to set these rates, and other FI securities.\n",
    "Daily volatility index (VIX) - for the reason described in the previous section.\n",
    "Composite indices - such NASDAQ and NYSE (from USA), FTSE100 (UK), Nikkei225 (Japan), Hang Seng and BSE Sensex (APAC) indices.\n",
    "Currencies - global trade is many times reflected into how currencies move, ergo we'll use a basket of currencies (such as USDJPY, GBPUSD, etc) as features.\n",
    "Overall, we have 72 other assets in the dataset - daily price for every asset.\n",
    "\n",
    "3.2. Tecnical indicators \n",
    "We already covered what are technical indicators and why we use them so let's jump straight to the code. We will create technical indicators only for GS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d243daf-f885-4268-a320-b5f49e0e1db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_technical_indicators(dataset):\n",
    "    # Create 7 and 21 days Moving Average\n",
    "    dataset['ma7'] = dataset['price'].rolling(window=7).mean()\n",
    "    dataset['ma21'] = dataset['price'].rolling(window=21).mean()\n",
    "    \n",
    "    # Create MACD\n",
    "    dataset['26ema'] = pd.ewma(dataset['price'], span=26)\n",
    "    dataset['12ema'] = pd.ewma(dataset['price'], span=12)\n",
    "    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n",
    "\n",
    "    # Create Bollinger Bands\n",
    "    dataset['20sd'] = pd.stats.moments.rolling_std(dataset['price'],20)\n",
    "    dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
    "    dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
    "    \n",
    "    # Create Exponential moving average\n",
    "    dataset['ema'] = dataset['price'].ewm(com=0.5).mean()\n",
    "    \n",
    "    # Create Momentum\n",
    "    dataset['momentum'] = dataset['price']-1\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537ec15-1da5-40e6-9933-3741f57037e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_TI_df = get_technical_indicators(dataset_ex_df[['GS']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455765f8-da8b-43b9-b124-987fc89419a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_TI_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1efd5-6e51-4f90-b6ab-2fc6b275709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_technical_indicators(dataset, last_days):\n",
    "    plt.figure(figsize=(16, 10), dpi=100)\n",
    "    shape_0 = dataset.shape[0]\n",
    "    xmacd_ = shape_0-last_days\n",
    "    \n",
    "    dataset = dataset.iloc[-last_days:, :]\n",
    "    x_ = range(3, dataset.shape[0])\n",
    "    x_ =list(dataset.index)\n",
    "    \n",
    "    # Plot first subplot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(dataset['ma7'],label='MA 7', color='g',linestyle='--')\n",
    "    plt.plot(dataset['price'],label='Closing Price', color='b')\n",
    "    plt.plot(dataset['ma21'],label='MA 21', color='r',linestyle='--')\n",
    "    plt.plot(dataset['upper_band'],label='Upper Band', color='c')\n",
    "    plt.plot(dataset['lower_band'],label='Lower Band', color='c')\n",
    "    plt.fill_between(x_, dataset['lower_band'], dataset['upper_band'], alpha=0.35)\n",
    "    plt.title('Technical indicators for Goldman Sachs - last {} days.'.format(last_days))\n",
    "    plt.ylabel('USD')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot second subplot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title('MACD')\n",
    "    plt.plot(dataset['MACD'],label='MACD', linestyle='-.')\n",
    "    plt.hlines(15, xmacd_, shape_0, colors='g', linestyles='--')\n",
    "    plt.hlines(-15, xmacd_, shape_0, colors='g', linestyles='--')\n",
    "    plt.plot(dataset['log_momentum'],label='Momentum', color='b',linestyle='-')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e9d05-60a7-4b6e-b793-797f352b66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_technical_indicators(dataset_TI_df, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a824e77-9c7c-4938-bf01-586493024d6f",
   "metadata": {},
   "source": [
    "\n",
    "3.3. Fundamental analysis \n",
    "       For fundamental analysis we will perform sentiment analysis on all daily news abou XXX. Using sigmoid the end result will be between 0 and 1. The closer the score is to 0 - the more negative the news is (closer to 1 indicates positive sentiment). For each day, we will create the average daily score (as a number between 0 and 1) and add it as a feature.\n",
    "\n",
    "3.3.1. Bidirectional Embedding Representations from Transformers - BERT \n",
    "       For the purpose of classifying news as positive or negative (or neutral) we will use BERT, which is a pre-trained language representation.\n",
    "\n",
    "       Pretrained BERT models are already available in MXNet/Gluon. We just need to instantiated them and add two (arbitrary number) Dense layers, going to softmax - the score is from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34aa7d2-d594-4cde-a56f-41c9ec421751",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# just import bert\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0483e-95d8-4ad1-ad9a-79e0f24b013c",
   "metadata": {},
   "source": [
    "\n",
    "3.4. Fourier transforms for trend analysis \n",
    "       Fourier transforms take a function and create a series of sine waves (with different amplitudes and frames). When combined, these sine waves approximate the original function. Mathematically speaking, the transforms look like this:\n",
    "\n",
    "$$G(f) = \\int_{-\\infty}^\\infty g(t) e^{-i 2 \\pi f t} dt$$\n",
    "We will Fourier transforms to extract global and local trends in the GS stock, and to also denoise it a little. So let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0225f-da95-4613-b39c-f55acd143046",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_FT = dataset_ex_df[['Date', 'GS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3998a59-cedb-49e6-bdd5-5786c423bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_fft = np.fft.fft(np.asarray(data_FT['GS'].tolist()))\n",
    "fft_df = pd.DataFrame({'fft':close_fft})\n",
    "fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))\n",
    "fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc0843-9bbf-48cd-b1bd-121f6a168f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7), dpi=100)\n",
    "fft_list = np.asarray(fft_df['fft'].tolist())\n",
    "for num_ in [3, 6, 9, 100]:\n",
    "    fft_list_m10= np.copy(fft_list); fft_list_m10[num_:-num_]=0\n",
    "    plt.plot(np.fft.ifft(fft_list_m10), label='Fourier transform with {} components'.format(num_))\n",
    "plt.plot(data_FT['GS'],  label='Real')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Figure 3: Goldman Sachs (close) stock prices & Fourier transforms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907b19d-8759-469f-af2a-91ac3a5b108c",
   "metadata": {},
   "source": [
    "       As you see in Figure 3 the more components from the Fourier transform we use the closer the approximation function is to the real stock price (the 100 components transform is almost identical to the original function - the red and the purple lines almost overlap). We use Fourier transforms for the purpose of extracting long- and short-term trends so we will use the transforms with 3, 6, and 9 components. You can infer that the transform with 3 components serves as the long term trend.\n",
    "\n",
    "       Another technique used to denoise data is call wavelets. Wavelets and Fourier transform gave similar results so we will only use Fourier transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74b40c-03fa-43b3-8138-1dbae4e12b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "items = deque(np.asarray(fft_df['absolute'].tolist()))\n",
    "items.rotate(int(np.floor(len(fft_df)/2)))\n",
    "plt.figure(figsize=(10, 7), dpi=80)\n",
    "plt.stem(items)\n",
    "plt.title('Figure 4: Components of Fourier transforms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a8811-aec8-4f1b-9898-1438b18220db",
   "metadata": {},
   "source": [
    "\n",
    "3.5. ARIMA as a feature \n",
    "       ARIMA is a technique for predicting time series data. We will show how to use it, and althouth ARIMA will not serve as our final prediction, we will use it as a technique to denoise the stock a little and to (possibly) extract some new patters or features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f9013-5624-491a-bc6b-967c10cd6895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pandas import DataFrame\n",
    "from pandas import datetime\n",
    "\n",
    "series = data_FT['GS']\n",
    "model = ARIMA(series, order=(5, 1, 0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb71c3-7867-489c-9c1b-ad6f638c8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(series)\n",
    "plt.figure(figsize=(10, 7), dpi=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa6a50-c217-4c8f-9532-98fd24954276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = series.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9fd1f-3f7d-48d5-bac0-8d6b046a9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18620e9c-3aaa-4db8-a828-7974a59731f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the predicted (from ARIMA) and real prices\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=100)\n",
    "plt.plot(test, label='Real')\n",
    "plt.plot(predictions, color='red', label='Predicted')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('USD')\n",
    "plt.title('Figure 5: ARIMA model on GS stock')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3fb65-93ae-4de6-828f-f95daf07baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "  As we can see from Figure 5 ARIMA gives a very good approximation of the real stock price. We will use the predicted price through ARIMA as an input feature into the LSTM because, as we mentioned before, we want to capture as many features and patterns about Goldman Sachs as possible. We go test MSE (mean squared error) of 10.151, which by itself is not a bad result (considering we do have a lot of test data), but still we will only use it as a feature in the LSTM.\n",
    "\n",
    "3.6. Statistical checks \n",
    "       Ensuring that the data has good quality is very important for out models. In order to make sure our data is suitable we will perform a couple of simple checks in order to ensure that the results we achieve and observe are indeed real, rather than compromised due to the fact that the underlying data distribution suffers from fundamental errors.\n",
    "\n",
    "3.6.1. Heteroskedasticity, multicollinearity, serial correlation \n",
    "Conditional Heteroskedasticity occurs when the error terms (the difference between a predicted value by a regression and the real value) are dependent on the data - for example, the error terms grow when the data point (along the x-axis) grow.\n",
    "Multicollinearity is when error terms (also called residuals) depend on each other.\n",
    "Serial correlation is when one data (feature) is a formula (or completely depemnds) of another feature.\n",
    "We will not go into the code here as it is straightforward and our focus is more on the deep learning parts, but the data is qualitative.\n",
    "\n",
    "3.7. Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930f599-4764-473e-81a8-8d7366906f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total dataset has {} samples, and {} features.'.format(dataset_total_df.shape[0], \\\n",
    "                                                              dataset_total_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5d4e3-848b-4469-8caf-aab3694f6b11",
   "metadata": {},
   "source": [
    " So, after adding all types of data (the correlated assets, technical indicators, fundamentl analysis, Fourier, and Arima) we have a total of 112 features for the 2,265 days (as mentioned before, however, only 1,585 days are for training data).\n",
    "\n",
    "       We will also have some more features generated from the autoencoders.\n",
    "\n",
    "3.7.1. Feature importance with XGBoost \n",
    "       Having so many features we have to consider whether all of them are really indicative of the direction GS stock will take. For example, we included USD denominated LIBOR rates in the dataset because we think that changes in LIBOR might indicate changes in the economy, that, in turn, might indicate changes in the GS's stock behavior. But we need to test. There are many ways to test feature importance, but the one we will apply uses XGBoost, because it gives one of the best results in both classification and regression problems.\n",
    "\n",
    "       Since the features dataset is quite large, for the purpose of presentation here we'll use only the technical indicators. During the real features importance testing all selected features proved somewhat important so we won't exclude anything when training the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9474c77-e95d-47e7-b5af-57f82249b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance_data(data_income):\n",
    "    data = data_income.copy()\n",
    "    y = data['price']\n",
    "    X = data.iloc[:, 1:]\n",
    "    \n",
    "    train_samples = int(X.shape[0] * 0.65)\n",
    " \n",
    "    X_train = X.iloc[:train_samples]\n",
    "    X_test = X.iloc[train_samples:]\n",
    "\n",
    "    y_train = y.iloc[:train_samples]\n",
    "    y_test = y.iloc[train_samples:]\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb06c55-7931-489d-ad63-c52b3c6ccf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test data\n",
    "(X_train_FI, y_train_FI), (X_test_FI, y_test_FI) = get_feature_importance_data(dataset_TI_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23ead2-1401-49d0-b738-e93a266d73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(gamma=0.0,n_estimators=150,base_score=0.7,colsample_bytree=1,learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae6c54-408c-4ebe-a958-11c61cb5ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbModel = regressor.fit(X_train_FI,y_train_FI, \\\n",
    "                         eval_set = [(X_train_FI, y_train_FI), (X_test_FI, y_test_FI)], \\\n",
    "                         verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772ca39-8649-4094-b1d2-1b7c0682135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = regressor.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd5a0da-9b6e-4f2f-b770-2d51b778deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rounds = range(len(eval_result['validation_0']['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24c665-d10b-4f58-b938-1e5bbe5e6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the training and validation errors in order to observe the training and check for overfitting (there isn't overfitting).\n",
    "\n",
    "plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\n",
    "plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training Vs Validation Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323c404-dff8-4e5c-a6df-671c71f1c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#can do with SHAP\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar([i for i in range(len(xgbModel.feature_importances_))], xgbModel.feature_importances_.tolist(), tick_label=X_test_FI.columns)\n",
    "plt.title('Figure 6: Feature importance of the technical indicators.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121262b-a989-4899-8cdc-204190bf06e1",
   "metadata": {},
   "source": [
    "Not surprisingly (for those with experience in stock trading) that MA7, MACD, and BB are among the important features.\n",
    "\n",
    "       I followed the same logic for performing feature importance over the whole dataset - just the training took longer and results were a little more difficult to read, as compared with just a handful of features.\n",
    "       \n",
    "       3.8. Extracting high-level features with Stacked Autoencoders \n",
    "Before we proceed to the autoencoders, we'll explore an alternative activation function.\n",
    "\n",
    "3.8.1. Activation function - GELU (Gaussian Error) \n",
    "       GELU - Gaussian Error Linear Unites was recently proposed - link. In the paper the authors show several instances in which neural networks using GELU outperform networks using ReLU as an activation. gelu is also used in BERT, the NLP approach we used for news sentiment analysis.\n",
    "\n",
    "       We will use GELU for the autoencoders.\n",
    "\n",
    "Note: The cell below shows the logic behing the math of GELU. It is not the actual implementation as an activation function. I had to implement GELU inside MXNet. If you follow the code and change act_type='relu' to act_type='gelu' it will not work, unless you change the implementation of MXNet. Make a pull request on the whole project to access the MXNet implementation of GELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d633d5-c442-4b6c-930a-c0cb5250687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3))))\n",
    "def relu(x):\n",
    "    return max(x, 0)\n",
    "def lrelu(x):\n",
    "    return max(0.01*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8ba47-399b-4831-a60b-75c3d41ece7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize GELU, ReLU, and LeakyReLU (the last one is mainly used in GANs - we also use it).\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=.5, hspace=None)\n",
    "\n",
    "ranges_ = (-10, 3, .25)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([i for i in np.arange(*ranges_)], [relu(i) for i in np.arange(*ranges_)], label='ReLU', marker='.')\n",
    "plt.plot([i for i in np.arange(*ranges_)], [gelu(i) for i in np.arange(*ranges_)], label='GELU')\n",
    "plt.hlines(0, -10, 3, colors='gray', linestyles='--', label='0')\n",
    "plt.title('Figure 7: GELU as an activation function for autoencoders')\n",
    "plt.ylabel('f(x) for GELU and ReLU')\n",
    "plt.xlabel('x')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([i for i in np.arange(*ranges_)], [lrelu(i) for i in np.arange(*ranges_)], label='Leaky ReLU')\n",
    "plt.hlines(0, -10, 3, colors='gray', linestyles='--', label='0')\n",
    "plt.ylabel('f(x) for Leaky ReLU')\n",
    "plt.xlabel('x')\n",
    "plt.title('Figure 8: LeakyReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b8af9-b037-4835-9f35-cf39ad53e6de",
   "metadata": {},
   "source": [
    "\n",
    "Note: In future versions of this notebook I will experiment using U-Net (link), and try to utilize the convolutional layer and extract (and create) even more features about the stock's underlying movement patterns. For now, we will just use a simple autoencoder made only from Dense layers.\n",
    "\n",
    "       Ok, back to the autoencoders, depicted below (the image is only schematic, it doesn't represent the real number of layers, units, etc.)\n",
    "\n",
    "Note: One thing that I will explore in a later version is removing the last layer in the decoder. Normally, in autoencoders the number of encoders == number of decoders. We want, however, to extract higher level features (rather than creating the same input), so we can skip the last layer in the decoder. We achieve this creating the encoder and decoder with same number of layers during the training, but when we create the output we use the layer next to the only one as it would contain the higher level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268c66c-dada-4179-b739-0ceccf9a74ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_batches = VAE_data.shape[0]/batch_size\n",
    "VAE_data = VAE_data.values\n",
    "\n",
    "train_iter = mx.io.NDArrayIter(data={'data': VAE_data[:num_training_days,:-1]}, \\\n",
    "                               label={'label': VAE_data[:num_training_days, -1]}, batch_size = batch_size)\n",
    "test_iter = mx.io.NDArrayIter(data={'data': VAE_data[num_training_days:,:-1]}, \\\n",
    "                              label={'label': VAE_data[num_training_days:,-1]}, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd26352-0f7d-404d-b80a-654aa9a589ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ctx =  mx.cpu()\n",
    "class VAE(gluon.HybridBlock):\n",
    "    def __init__(self, n_hidden=400, n_latent=2, n_layers=1, n_output=784, \\\n",
    "                 batch_size=100, act_type='relu', **kwargs):\n",
    "        self.soft_zero = 1e-10\n",
    "        self.n_latent = n_latent\n",
    "        self.batch_size = batch_size\n",
    "        self.output = None\n",
    "        self.mu = None\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.encoder = nn.HybridSequential(prefix='encoder')\n",
    "            \n",
    "            for i in range(n_layers):\n",
    "                self.encoder.add(nn.Dense(n_hidden, activation=act_type))\n",
    "            self.encoder.add(nn.Dense(n_latent*2, activation=None))\n",
    "\n",
    "            self.decoder = nn.HybridSequential(prefix='decoder')\n",
    "            for i in range(n_layers):\n",
    "                self.decoder.add(nn.Dense(n_hidden, activation=act_type))\n",
    "            self.decoder.add(nn.Dense(n_output, activation='sigmoid'))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        h = self.encoder(x)\n",
    "        #print(h)\n",
    "        mu_lv = F.split(h, axis=1, num_outputs=2)\n",
    "        mu = mu_lv[0]\n",
    "        lv = mu_lv[1]\n",
    "        self.mu = mu\n",
    "\n",
    "        eps = F.random_normal(loc=0, scale=1, shape=(self.batch_size, self.n_latent), ctx=model_ctx)\n",
    "        z = mu + F.exp(0.5*lv)*eps\n",
    "        y = self.decoder(z)\n",
    "        self.output = y\n",
    "\n",
    "        KL = 0.5*F.sum(1+lv-mu*mu-F.exp(lv),axis=1)\n",
    "        logloss = F.sum(x*F.log(y+self.soft_zero)+ (1-x)*F.log(1-y+self.soft_zero), axis=1)\n",
    "        loss = -logloss-KL\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688453c1-5d58-4358-a560-5058872cb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden=400 # neurons in each layer\n",
    "n_latent=2 \n",
    "n_layers=3 # num of dense layers in encoder and decoder respectively\n",
    "n_output=VAE_data.shape[1]-1 \n",
    "\n",
    "net = VAE(n_hidden=n_hidden, n_latent=n_latent, n_layers=n_layers, n_output=n_output, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60ec0f6-6bf4-46c3-96e6-20d4f7399ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu())\n",
    "net.hybridize()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': .01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a2e80-83fb-49d5-b203-ba07e0420a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "#So we have 3 layers (with 400 neurons in each) in both the encoder and the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723dac9-bb18-4fb4-948e-453116cd658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 150\n",
    "print_period = n_epoch // 10\n",
    "start = time.time()\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "for epoch in range(n_epoch):\n",
    "    epoch_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "\n",
    "    train_iter.reset()\n",
    "    test_iter.reset()\n",
    "\n",
    "    n_batch_train = 0\n",
    "    for batch in train_iter:\n",
    "        n_batch_train +=1\n",
    "        data = batch.data[0].as_in_context(mx.cpu())\n",
    "\n",
    "        with autograd.record():\n",
    "            loss = net(data)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        epoch_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "    n_batch_val = 0\n",
    "    for batch in test_iter:\n",
    "        n_batch_val +=1\n",
    "        data = batch.data[0].as_in_context(mx.cpu())\n",
    "        loss = net(data)\n",
    "        epoch_val_loss += nd.mean(loss).asscalar()\n",
    "\n",
    "    epoch_loss /= n_batch_train\n",
    "    epoch_val_loss /= n_batch_val\n",
    "\n",
    "    training_loss.append(epoch_loss)\n",
    "    validation_loss.append(epoch_val_loss)\n",
    "\n",
    "    \"\"\"if epoch % max(print_period, 1) == 0:\n",
    "        print('Epoch {}, Training loss {:.2f}, Validation loss {:.2f}'.\\\n",
    "              format(epoch, epoch_loss, epoch_val_loss))\"\"\"\n",
    "\n",
    "end = time.time()\n",
    "print('Training completed in {} seconds.'.format(int(end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4938abf-67c6-4069-9dbc-c63e684658f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_total_df['Date'] = dataset_ex_df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163eb17-eb23-4c80-b194-8dcd626d5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_added_df = mx.nd.array(dataset_total_df.iloc[:, :-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0673b-ed67-4103-b16c-63f958fd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The shape of the newly created (from the autoencoder) features is {}.'.format(vae_added_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a5016-ba07-4326-b6d7-86035175618a",
   "metadata": {},
   "source": [
    "   We created 112 more features from the autoencoder. As we want to only have high level features (overall patterns) we will create an Eigen portfolio on the newly created 112 features using Principal Component Analysis (PCA). This will reduce the dimension (number of columns) of the data. The descriptive capability of the Eigen portfolio will be the same as the original 112 features.\n",
    "\n",
    "Note Once again, this is purely experimental. I am not 100% sure the described logic will hold. As everything else in AI and deep learning, this is art and needs experiments.\n",
    "\n",
    "3.8.2. Eigen portfolio with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd9ce7-911e-4d7c-a5b8-0d199bc82cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the PCA to create the new components to explain 80% of the variance\n",
    "pca = PCA(n_components=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b45673-6ea7-43e0-b97d-10c4dfeef61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = StandardScaler().fit_transform(vae_added_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bd731-360c-43df-b29e-410a765b9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents = pca.fit_transform(x_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38acf614-c8e2-4e45-8109-b375819f10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents.n_components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9933775-54f9-45bd-9f3d-e08187437557",
   "metadata": {},
   "source": [
    " So, in order to explain 80% of the variance we need 84 (out of the 112) features. This is still a lot. So, for now we will not include the autoencoder created features. I will work on creating the autoencoder architecture in which we get the output from an intermediate layer (not the last one) and connect it to another Dense layer with, say, 30 neurons. Thus, we will 1) only extract higher level features, and 2) come up with significantly fewer number of columns.\n",
    "\n",
    "3.9. Deep Unsupervised Learning for anomaly detection in derivatives pricing \n",
    "-- To be added soon.\n",
    "\n",
    "4. Generative Adversarial Network (GAN) \n",
    "Figure 9: Simple GAN architecture\n",
    "\n",
    "How GANs work?\n",
    "       As mentioned before, the purpose of this notebook is not to explain in detail the math behind deep learning but to show its applications. Of course, thorough and very solid understanding from the fundamentals down to the smallest details, in my opinion, is extremely imperative. Hence, we will try to balance and give a high-level overview of how GANs work in order for the reader to fully understand the rationale behind using GANs in predicting stock price movements. Feel free to skip this and the next section if you are experienced with GANs (and do check section 4.2.).\n",
    "\n",
    "A GAN network consists of two models - a Generator ($G$) and Discriminator ($D$). The steps in training a GAN are:\n",
    "\n",
    "The Generator is, using random data (noise denoted $z$), trying to 'generate' data indistinguishable of, or extremely close to, the real data. It's purpose is to learn the distribution of the real data.\n",
    "Randomly, real or generated data is fitted into the Discriminator, which acts as a classifier and tries to understand whether the data is coming from the Generator or is the real data. $D$ estimates the (distributions) probabilities of the incoming sample to the real dataset. (more info on comparing two distributions in section 4.2. below).\n",
    "Then, the losses from $G$ and $D$ are combined and propagated back through the generator. Ergo, the generator's loss depends on both the generator and the discriminator. This is the step that helps the Generator learn about the real data distribution. If the generator doesn't do a good job at generating a realistic data (having the same distribution), the Discriminator's work will be very easy to distinguish generated from real data sets. Hence, the Discriminator's loss will be very small. Small discriminator loss will result in bigger generator loss (see the equation below for $L(D, G)$). This makes creating the discriminator a bit tricky, because too good of a discriminator will always result in a huge generator loss, making the generator unable to learn.\n",
    "The process goes on until the Discriminator can no longer distinguish generated from real data.\n",
    "\n",
    "When combined together, $D$ and $G$ as sort of playing a minmax game (the Generator is trying to fool the Discriminator making it increase the probability for on fake examples, i.e. minimize $\\mathbb{E}_{z \\sim p_{z}(z)} [\\log (1 - D(G(z)))]$. The Discriminator wants to separate the data coming from the Generator, $D(G(z))$, by maximizing $\\mathbb{E}_{x \\sim p_{r}(x)} [\\log D(x)]$). Having separated loss functions, however, it is not clear how both can converge together (that is why we use some advancements over the plain GANs, such as Wasserstein GAN). Overall, the combined loss function looks like:\n",
    "\n",
    "$$L(D, G) = \\mathbb{E}_{x \\sim p_{r}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)} [\\log(1 - D(G(z)))]$$\n",
    "Note: Really useful tips for training GANs can be found here.\n",
    "\n",
    "Note: I will not include the complete code behind the GAN and the Reinforcement learning parts in this notebook - only the results from the execution (the cell outputs) will be shown. Make a pull request or contact me for the code.\n",
    "\n",
    "4.1. Why GAN for stock market prediction? \n",
    "       Generative Adversarial Networks (GAN) have been recently used mainly in creating realistic images, paintings, and video clips. There aren't many applications of GANs being used for predicting time-series data as in our case. The main idea, however, should be same - we want to predict future stock movements. In the future, the pattern and behavior of GS's stock should be more or less the same (unless it starts operating in a totally different way, or the economy drastically changes). Hence, we want to 'generate' data for the future that will have similar (not absolutely the same, of course) distribution as the one we already have - the historical trading data. So, in theory, it should work.   In our case, we will use LSTM as a time-series generator, and CNN as a discriminator.\n",
    "\n",
    "4.2. Metropolis-Hastings GAN and Wasserstein GAN \n",
    "Note: The next couple of sections assume some experience with GANs.\n",
    "\n",
    "I. Metropolis-Hastings GAN\n",
    "       A recent improvement over the traditional GANs came out from Uber's engineering team and is called Metropolis-Hastings GAN (MHGAN). The idea behind Uber's approach is (as they state it) somewhat similar to another approach created by Google and University of California, Berkeley called Discriminator Rejection Sampling (DRS). Basically, when we train GAN we use the Discriminator ($D$) for the sole purpose of better training the Generator ($G$). Often, after training the GAN we do not use the $D$ any more. MHGAN and DRS, however, try to use $D$ in order to choose samples generated by $G$ that are close to the real data distribution (slight difference between is that MHGAN uses Markov Chain Monte Carlo (MCMC) for sampling).\n",
    "\n",
    "       MHGAN takes K samples generated from the $G$ (created from independent noise inputs to the $G$ - $z_0$ to $z_K$ in the figure below). Then it sequentially runs through the K outputs ($x'_0$ to $x'_K$) and following an acceptance rule (created from the Discriminator) decides whether to accept the current sample or keep the last accepted one. The last kept output is the one considered the real output of $G$.\n",
    "\n",
    "Note: MHGAN is originally implemented by Uber in pytorch. I only transferred it into MXNet/Gluon.\n",
    "\n",
    "Note: I will also upload it into Github sometime soon.\n",
    "\n",
    "Figure 10: Visual representation of MHGAN (from the original Uber post).\n",
    "\n",
    "II. Wasserstein GAN \n",
    "       Training GANs is quite difficult. Models may never converge and mode collapse can easily happen. We will use a modification of GAN called Wasserstein GAN - WGAN.\n",
    "\n",
    " Again, we will not go into details, but the most notable points to make are:\n",
    "\n",
    "As we know the main goal behind GANs is for the Generator to start transforming random noise into some given data that we want to mimic. Ergo, the idea of comparing the similarity between two distributions is very imperative in GANs. The two most widely used such metrics are:\n",
    "KL divergence (Kullback–Leibler) - $D_{KL}(p \\| q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$. $D_{KL}$ is zero when $p(x)$ is equal to $q(x)$,\n",
    "JS Divergence (Jensen–Shannon) - $D_{JS}(p \\| q) = \\frac{1}{2} D_{KL}(p \\| \\frac{p + q}{2}) + \\frac{1}{2} D_{KL}(q \\| \\frac{p + q}{2})$. JS divergence is bounded by 0 and 1, and, unlike KL divergence, is symmetric and smoother. Significant success in GAN training was achieved when the loss was switched from KL to JS divergence.\n",
    "WGAN uses Wasserstein distance, $W(p_r, p_g) = \\frac{1}{K} \\sup_{\\| f \\|_L \\leq K} \\mathbb{E}_{x \\sim p_r}[f(x)] - \\mathbb{E}_{x \\sim p_g}[f(x)]$ (where $sup$ stands for supremum), as a loss function (also called Earth Mover's distance, because it normally is interpreted as moving one pile of, say, sand to another one, both piles having different probability distributions, using minimum energy during the transformation). Compared to KL and JS divergences, Wasserstein metric gives a smooth measure (without sudden jumps in divergence). This makes it much more suitable for creating a stable learning process during the gradient descent.\n",
    "Also, compared to KL and JS, Wasserstein distance is differentiable nearly everywhere. As we know, during backpropagation, we differentiate the loss function in order to create the gradients, which in turn update the weights. Therefore, having a differentiable loss function is quite important.\n",
    "Hands down, this was the toughest part of this notebook. Mixing WGAN and MHGAN took me three days.\n",
    "4.4. The Generator - One layer RNN \n",
    "4.4.1. LSTM or GRU \n",
    "       As mentioned before, the generator is a LSTM network a type of Recurrent Neural Network (RNN). RNNs are used for time-series data because because they keep track of all previous data points and can capture patterns developing through time. Due to their nature, RNNs many time suffer from vanishing gradient - that is, the changes the weights receive during training become so small, that they don't change, making the network unable to converge to a minimal loss (The opposite problem can also be observed at times - when gradients become too big. This is called gradient exploding, but the solution to this is quite simple - clip gradients if they start exceeding some constant number, i.e. gradient clipping). Two modifications tackle this problem - Gated Recurrent Unit (GRU) and Long-Short Term Memory (LSTM). The biggest differences between the two are: 1) GRU has 2 gates (update and reset) and LSTM has 4 (update, input, forget, and output), 2) LSTM maintains an internal memory state, while GRU doesn’t, and 3) LSTM applies a nonlinearity (sigmoid) before the output gate, GRU doesn’t.\n",
    "\n",
    "       In most cases LSTM and GRU give similar results in terms of accuracy but GRU is much less computational intensive, as GRU has much fewer trainable params. LSTMs, however, and much more used.\n",
    "\n",
    "       Strictly speaking, the math behind the LSTM cell (the gates) is:\n",
    "       \n",
    "       $$g_t = \\text{tanh}(X_t W_{xg} + h_{t-1} W_{hg} + b_g),$$$$i_t = \\sigma(X_t W_{xi} + h_{t-1} W_{hi} + b_i),$$$$f_t = \\sigma(X_t W_{xf} + h_{t-1} W_{hf} + b_f),$$$$o_t = \\sigma(X_t W_{xo} + h_{t-1} W_{ho} + b_o),$$$$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t,$$$$h_t = o_t \\odot \\text{tanh}(c_t),$$\n",
    "where $\\odot$ is an element-wise multiplication operator, and, for all $x = [x_1, x_2, \\ldots, x_k]^\\top \\in R^k$ the two activation functions:,\n",
    "\n",
    "$$\\sigma(x) = \\left[\\frac{1}{1+\\exp(-x_1)}, \\ldots, \\frac{1}{1+\\exp(-x_k)}]\\right]^\\top,$$$$\\text{tanh}(x) = \\left[\\frac{1-\\exp(-2x_1)}{1+\\exp(-2x_1)},  \\ldots, \\frac{1-\\exp(-2x_k)}{1+\\exp(-2x_k)}\\right]^\\top$$\n",
    "4.4.2. The LSTM architecture \n",
    "       The LSTM architecture is very simple - one LSTM layer with 112 input units (as we have 112 features in the dataset) and 500 hidden units, and one Dense layer with 1 output - the price for every day. The initializer is Xavier and we will use L1 loss (which is mean absolute error loss with L1 regularization - see section 4.4.5. for more info on regularization).\n",
    "\n",
    "Note - In the code you can see we use Adam (with learning rate of .01) as an optimizer. Don't pay too much attention on that now - there is a section specially dedicated to explaoin what hyperparameters we use (learning rate is excluded as we have learning rate scheduler - section 4.4.3.) and how we optimize these hyperparaments - section 4.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc43496-002c-4ca8-9128-f15698a43a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_num_features = dataset_total_df.shape[1]\n",
    "sequence_length = 17\n",
    "\n",
    "class RNNModel(gluon.Block):\n",
    "    def __init__(self, num_embed, num_hidden, num_layers, bidirectional=False, \\\n",
    "                 sequence_length=sequence_length, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.num_hidden = num_hidden\n",
    "        with self.name_scope():\n",
    "            self.rnn = rnn.LSTM(num_hidden, num_layers, input_size=num_embed, \\\n",
    "                                bidirectional=bidirectional, layout='TNC')\n",
    "            \n",
    "            self.decoder = nn.Dense(1, in_units=num_hidden)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        output, hidden = self.rnn(inputs, hidden)\n",
    "        decoded = self.decoder(output.reshape((-1, self.num_hidden)))\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)\n",
    "    \n",
    "lstm_model = RNNModel(num_embed=gan_num_features, num_hidden=500, num_layers=1)\n",
    "lstm_model.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu())\n",
    "trainer = gluon.Trainer(lstm_model.collect_params(), 'adam', {'learning_rate': .01})\n",
    "loss = gluon.loss.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7348bc-dd2a-4a80-a2cb-cb568fa603df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use 500 neurons in the LSTM layer and use Xavier initialization. For regularization we'll use L1. Let's see what's inside the LSTM as printed by MXNet.\n",
    "print(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da84a0-2ee8-4298-96dd-d0296de56ad9",
   "metadata": {},
   "source": [
    " As we can see, the unput of the LSTM are the 112 features (dataset_total_df.shape[1]) which then go into 500 neurons in the LSTM layer, and then transformed to a single output - the stock price value.\n",
    "\n",
    "       The logic behind the LSTM is: we take 17 (sequence_length) days of data (again, the data being the stock price for GS stock every day + all the other feature for that day - correlated assets, sentiment, etc.) and try to predict the 18th day.\n",
    "\n",
    "       In another post I will explore whether modification over the vanilla LSTM would be more beneficial, such as:\n",
    "\n",
    "using bidirectional LSTM layer - in theory, going backwards (from end of the data set towards the beginning) might somehow help the LSTM figure out the pattern of the stock movement.\n",
    "using stacked RNN architecture - having not only one LSTM layer but 2 or more. This, however, might be dangerous, as we might end up overfitting the model, as we don't have a lot of data (we have just 1,585 day worth of data).\n",
    "Exploring GRU - as already explained, GRUs' cells are much more simpler.\n",
    "Adding attention vectors to the RNN.\n",
    "4.4.3. Learning rate scheduler \n",
    "       One of the most important hyperparameters is the learning rate. Setting the learning rate for almost every optimizer (such as SGD, Adam, or RMSProp) is crucially important when training neural networks because it controls both the speed of convergence and the ultimate performance of the network. One of the simplest learning rate strategies is to have a fixed learning rate throughout the training process. Choosing a small learning rate allows the optimizer find good solutions, but this comes at the expense of limiting the initial speed of convergence. Changing the learning rate over time can overcome this tradeoff.\n",
    "\n",
    "       Recent papers, such as this, show the benefits of changing the global learning rate during training, in terms of both convergence and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba390b9f-90b5-4d3e-addb-209240f8984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangularSchedule():\n",
    "    def __init__(self, min_lr, max_lr, cycle_length, inc_fraction=0.5):     \n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.cycle_length = cycle_length\n",
    "        self.inc_fraction = inc_fraction\n",
    "        \n",
    "    def __call__(self, iteration):\n",
    "        if iteration <= self.cycle_length*self.inc_fraction:\n",
    "            unit_cycle = iteration * 1 / (self.cycle_length * self.inc_fraction)\n",
    "        elif iteration <= self.cycle_length:\n",
    "            unit_cycle = (self.cycle_length - iteration) * 1 / (self.cycle_length * (1 - self.inc_fraction))\n",
    "        else:\n",
    "            unit_cycle = 0\n",
    "        adjusted_cycle = (unit_cycle * (self.max_lr - self.min_lr)) + self.min_lr\n",
    "        return adjusted_cycle\n",
    "\n",
    "class CyclicalSchedule():\n",
    "    def __init__(self, schedule_class, cycle_length, cycle_length_decay=1, cycle_magnitude_decay=1, **kwargs):\n",
    "        self.schedule_class = schedule_class\n",
    "        self.length = cycle_length\n",
    "        self.length_decay = cycle_length_decay\n",
    "        self.magnitude_decay = cycle_magnitude_decay\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def __call__(self, iteration):\n",
    "        cycle_idx = 0\n",
    "        cycle_length = self.length\n",
    "        idx = self.length\n",
    "        while idx <= iteration:\n",
    "            cycle_length = math.ceil(cycle_length * self.length_decay)\n",
    "            cycle_idx += 1\n",
    "            idx += cycle_length\n",
    "        cycle_offset = iteration - idx + cycle_length\n",
    "        \n",
    "        schedule = self.schedule_class(cycle_length=cycle_length, **self.kwargs)\n",
    "        return schedule(cycle_offset) * self.magnitude_decay**cycle_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20551851-e07e-43c0-8d71-56a6d85361d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = CyclicalSchedule(TriangularSchedule, min_lr=0.5, max_lr=2, cycle_length=500)\n",
    "iterations=1500\n",
    "\n",
    "plt.plot([i+1 for i in range(iterations)],[schedule(i) for i in range(iterations)])\n",
    "plt.title('Learning rate for each epoch')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd296a-0600-4db4-a1b3-b8cdd185ed38",
   "metadata": {},
   "source": [
    "4.4.4. How to prevent overfitting and the bias-variance trade-off \n",
    "       Having a lot of features and neural networks we need to make sure we prevent overfitting and be mindful of the total loss.\n",
    "\n",
    "We use several techniques for preventing overfitting (not only in the LSTM, but also in the CNN and the auto-encoders):\n",
    "\n",
    "Ensuring data quality. We already performed statistical checks and made sure the data doesn't suffer from multicollinearity or serial autocorrelation. Further we performed feature importance check on each feature. Finally, the initial feature selection (e.g. selecting correlated assets, technical indicators, etc.) was done with some domain knowledge about the mechanics behind the way stock markets work.\n",
    "Regularization (or weights penalty). The two most widely used regularization techniques are LASSO (L1) and Ridge (L2). L1 adds the mean absolute error and L2 adds mean squared error to the loss. Without going into too many mathematical details, the basic differences are: lasso regression (L1) does both variable selection and parameter shrinkage, whereas Ridge regression only does parameter shrinkage and end up including all the coefficients in the model. In presence of correlated variables, ridge regression might be the preferred choice. Also, ridge regression works best in situations where the least square estimates have higher variance. Therefore, it depends on our model objective. The impact of the two types of regularizations is quite different. While they both penalize large weights, L1 regularization leads to a non-differentiable function at zero. L2 regularization favors smaller weights, but L1 regularization favors weights that go to zero. So, with L1 regularization you can end up with a sparse model - one with fewer parameters. In both cases the parameters of the L1 and L2 regularized models \"shrink\", but in the case of L1 regularization the shrinkage directly impacts the complexity (the number of parameters) of the model. Precisely, ridge regression works best in situations where the least square estimates have higher variance. L1 is more robust to outliers, is used when data is sparse, and creates feature importance. We will use L1.\n",
    "Dropout. Dropout layers randomly remove nodes in the hidden layers.\n",
    "Dense-sparse-dense training. - link\n",
    "Early stopping.\n",
    "       Another important consideration when building complex neural networks is the bias-variance trade-off. Basically, the error we get when training nets is a function of the bias, the variance, and irreducible error - σ (error due to noise and randomness). The simplest formula of the trade-off is:\n",
    "\n",
    "$$Error = bias^{2} + variance + \\sigma$$\n",
    "Bias. Bias measures how well a trained (on training dataset) algorithm can generalize on unseen data. High bias (underfitting) meaning the model cannot work well on unseen data.\n",
    "Variance. Variance measures the sensitivity of the model to changes in the dataset. High variance is the overfitting.\n",
    "4.4.5. Custom weights initializers and custom loss metric \n",
    "Coming soon\n",
    "\n",
    "4.5. The Discriminator - One Dimentional CNN \n",
    "4.5.1. Why CNN as a discriminator? \n",
    "       We usually use CNNs for work related to images (classification, context extraction, etc). They are very powerful at extracting features from features from features, etc. For example, in an image of a dog, the first convolutional layer will detect edges, the second will start detecting circles, and the third will detect a nose. In our case, data points form small trends, small trends form bigger, trends in turn form patterns. CNNs' ability to detect features can be used for extracting information about patterns in GS's stock price movements.\n",
    "\n",
    "       Another reason for using CNN is that CNNs work well on spatial data - meaning data points that are closer to each other are more related to each other, than data points spread across. This should hold true for time series data. In our case each data point (for each feature) is for each consecutive day. It is natural to assume that the closer two days are to each other, the more related they are to each other. One thing to consider (although not covered in this work) is seasonality and how it might change (if at all) the work of the CNN.\n",
    "\n",
    "Note: As many other parts in this notebook, using CNN for time series data is experimental. We will inspect the results, without providing mathematical or other proofs. And results might vary using different data, activation functions, etc.\n",
    "\n",
    "4.5.1. The CNN Architecture \n",
    "Figure 11: High level overview of the CNN architecture.\n",
    "\n",
    "The code for the CNN inside the GAN looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f6c9c-3f25-4546-b8b4-8edb872377bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fc = 512\n",
    "\n",
    "# ... other parts of the GAN\n",
    "\n",
    "cnn_net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    \n",
    "    # Add the 1D Convolutional layers\n",
    "    cnn_net.add(gluon.nn.Conv1D(32, kernel_size=5, strides=2))\n",
    "    cnn_net.add(nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(gluon.nn.Conv1D(64, kernel_size=5, strides=2))\n",
    "    cnn_net.add(nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(nn.BatchNorm())\n",
    "    cnn_net.add(gluon.nn.Conv1D(128, kernel_size=5, strides=2))\n",
    "    cnn_net.add(nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(nn.BatchNorm())\n",
    "    \n",
    "    # Add the two Fully Connected layers\n",
    "    cnn_net.add(nn.Dense(220, use_bias=False), nn.BatchNorm(), nn.LeakyReLU(0.01))\n",
    "    cnn_net.add(nn.Dense(220, use_bias=False), nn.Activation(activation='relu'))\n",
    "    cnn_net.add(nn.Dense(1))\n",
    "    \n",
    "# ... other parts of the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b5e71-8d53-4e9a-9b00-7a6728d7d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's print the CNN.\n",
    "print(cnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350eb96e-cdbe-4a8e-9c50-cd5e05c139bb",
   "metadata": {},
   "source": [
    "\n",
    "4.6. Hyperparameters \n",
    "The hyperparameters that we will track and optimize are:\n",
    "\n",
    "batch_size : batch size of the LSTM and CNN\n",
    "cnn_lr: the learningrate of the CNN\n",
    "strides: the number of strides in the CNN\n",
    "lrelu_alpha: the alpha for the LeakyReLU in the GAN\n",
    "batchnorm_momentum: momentum for the batch normalisation in the CNN\n",
    "padding: the padding in the CNN\n",
    "kernel_size':1: kernel size in the CNN\n",
    "dropout: dropout in the LSTM\n",
    "filters: the initial number of filters\n",
    "We will train over 200 epochs.\n",
    "\n",
    "5. Hyperparameters optimization \n",
    "       After the GAN trains on the 200 epocgs it will record the MAE (which is the error function in the LSTM, the $G$) and pass it as a reward value to the Reinforcement learning that will decide whether to change the hyperparameters of keep training with the same set of hyperparameters. As described later, this approach is strictly for experimenting with RL.\n",
    "\n",
    "       If the RL decides it will update the hyperparameters it will call Bayesian optimisation (discussed below) library that will give the next best expected set of the hyperparams.\n",
    "\n",
    "5.1. Reinforcement learning for hyperparameters optimization \n",
    "       Why do we use reinforcement learning in the hyperparameters optimization? Stock markets change all the time. Even if we manage to train our GAN and LSTM to create extremely accurate results, the results might only be valid for a certain period. Meaning, we need to constantly optimise the whole process. To optimize the process we can:\n",
    "\n",
    "Add or remove features (e.g. add new stocks or currencies that might be correlated)\n",
    "Improve the our deep learning models. One of the most important ways to improve the models is through the hyper parameters (listed in Section 5). Once having found a certain set of hyperparameters we need to decide when to change them and when to use the already known set (exploration vs. exploitation). Also, stocks market represents a continuous space that depends on millions parameters.\n",
    "\n",
    "Note: The purpose of the whole reinforcement learning part of this notebook is more research oriented. We will explore different RL approaches using the GAN as an environment. There are many ways in which we can successfully perform hyperparameter optimization on our deep learning models without using RL. But... why not.\n",
    "\n",
    "Note: The next several sections assume you have some knowledge about RL - especially policy methods and Q-learning.\n",
    "\n",
    "5.1.1. Reinforcement Learning Theory \n",
    "       Without explaining the basics of RL we will jump into the details of the specific approaches we implement here. We will use model-free RL algorithms for the obvious reason that we do not know the whole environment, hence there is no defined model for how the environment works - if there was we wouldn't need to predict stock prices movements - they will just follow the model. We will use the two subdivisions of model-free RL - Policy optimization and Q-learning.\n",
    "\n",
    "Q-learning - in Q-learning we learn the value of taking an action from a given state. Q-value is the expected return after taking the action. We will use Rainbow which is a combination of seven Q learning algorithms.\n",
    "Policy Optimization - in policy optimization we learn the action to take from a given state. (if we use methods like Actor/Critic) we also learn the value of being in a given state. We will use Proximal Policy Optimization.\n",
    "       One crucial aspect of building a RL algorithm is accurately setting the reward. It has to capture all aspects of the environment and the agent's interaction with the environment. We define the reward, R, as:\n",
    "\n",
    "$$Reward = 2*loss_G + loss_D + accuracy_G,$$\n",
    "where $loss_G$, $accuracy_G$, and $loss_D$ are the Generator's loss and accuracy, and Discriminator's loss, respectively. The environment is the GAN and the results of the LSTM training. The action the different agents can take is how to change the hyperparameters of the GAN's $D$ and $G$ nets.\n",
    "\n",
    "5.1.1.1. Rainbow \n",
    "What is Rainbow?\n",
    "\n",
    "       Rainbow (link) is a Q learning based off-policy deep reinforcement learning algorithm combining seven algorithm together:\n",
    "\n",
    "DQN. DQN is an extension of Q learning algorithm that uses a neural network to represent the Q value. Similar to supervised (deep) learning, in DQN we train a neural network and try to minimize a loss function. We train the network by randomly sampling transitions (state, action, reward). The layers can be not only fully connected ones, but also convolutional, for example.\n",
    "Double Q Learning. Double QL handles a big problem in Q learning, namely the overestimation bias.\n",
    "Prioritized replay. In the vanilla DQN, all transitions are stored in a replay buffer and it uniformly samples this buffer. However, not all transitions are equally beneficial during the learning phase (which also makes learning inefficient as more episodes are required). Prioritized experience replay doesn't sample uniformly, rather it uses a distribution that gives higher probability to samples that have had higher Q loss in previous iterations.\n",
    "\n",
    "Dueling networks. Dueling networks change the Q learning architecture a little by using two separate streams (i.e. having two different mini-neural networks). One stream is for the value and one for the advantage. Both of them share a convolutional encoder. The tricky part is the merging of the streams - it uses a special aggregator (Wang et al. 2016).\n",
    "Advantage, formula is $A(s, a) = Q(s, a) - V(s)$, generally speaking is a comparison of how good an action is compared to the average action for a specific state. Advantages are sometimes used when a 'wrong' action cannot be penalized with negative reward. So advantage will try to further reward good actions from the average actions.\n",
    "Multi-step learning. The big difference behind Multi-step learning is that it calculates the Q-values using N-step returns (not only the return from the next step), which naturally should be more accurate.\n",
    "Distributional RL. Q learning uses average estimated Q-value as target value. However, in many cases the Q-values might not be the same in different situations. Distributional RL can directly learn (or approximate) the distribution of Q-values rather than averaging them. Again, the math is much more complicated than that, but for us the benefit is more accurate sampling of the Q-values.\n",
    "Noisy Nets. Basic DQN implements a simple 𝜀-greedy mechanism to do exploration. This approach to exploration inefficient at times. The way Noisy Nets approach this issue is by adding a noisy linear layer. Over time, the network will learn how to ignore the noise (added as a noisy stream). But this learning comes at different rates in different parts of the space, allowing for state exploration.\n",
    "#### Note: Stay tuned - I will upload a MXNet/Gluon implementation on Rainbow to Github in early February 2019.\n",
    "5.1.1.2. PPO \n",
    "       Proximal Policy Optimization (PPO) is a policy optimization model-free type of reinforcement learning. It is much simpler to implement that other algorithms and gives very good results.\n",
    "\n",
    "       Why do we use PPO? One of the advantages of PPO is that it directly learns the policy, rather than indirectly via the values (the way Q Learning uses Q-values to learn the policy). It can work well in continuous action spaces, which is suitable in our use case and can learn (through mean and standard deviation) the distribution probabilities (if softmax is added as an output).\n",
    "\n",
    "       The problem of policy gradient methods is that they are extremely sensitive to the step size choice - if it is small the progress takes too long (most probably mainly due to the need of a second-order derivatives matrix); if it is large, there is a lot noise which significantly reduces the performance. Input data is nonstationary due to the changes in the policy (also the distributions of the reward and observations change). As compared to supervised learning, poorly chosen step can be much more devastating as it affects the whole distribution of next visits. PPO can solve these issues. What is more, compared to some other approaches, PPO:\n",
    "\n",
    "is much less complicated, for example compared to ACER, which requires additional code for keeping the off-policy correlations and also a replay buffer, or TRPO which has a constraint imposed on the surrogate objective function (the KL divergence between the old and the new policy). This constraint is used to control the policy of changing too much - which might create instability. PPO reduces the computation (created by the constraint) by utilizing a clipped (between [1- 𝜖, 1+𝜖]) surrogate objective function and modifying the objective function with a penalty for having too big of an update.\n",
    "gives compatibility with algos that share parameters between value and policy function or auxiliary losses, as compared to TRPO (although PPO also have the gain of trust region PO).\n",
    "\n",
    "Note: For the purpose of our exercise we won't go too much into the research and optimization of RL approaches, PPO and the others included. Rather, we will take what is available and try to fit into our process for hyperparameter optimization for our GAN, LSTM, and CNN models. The code we will reuse and customize is created by OpenAI and is available here.\n",
    "\n",
    "5.1.2. Further work on Reinforcement learning \n",
    "Some ideas for further exploring reinforcement learning:\n",
    "\n",
    "One of the first things I will introduce next is using Augmented Random Search (link) as an alternative algorithm. The authors of the algorithm (out of UC, Berkeley) have managed to achive similar rewards results as other state of the art approaches, such as PPO, but on average 15 times faster.\n",
    "Choosing a reward function is very important. I stated the currently used reward function above, but I will try to play with different functions as an alternative.\n",
    "Using Curiosity as an exploration policy.\n",
    "Create multi-agent architecture as proposed by Berkeley's AI Research team (BAIR) - link.\n",
    "5.2. Bayesian optimization \n",
    "       Instead of the grid search, that can take a lot of time to find the best combination of hyperparameters, we will use Bayesian optimization. The library that we'll use is already implemented - link.\n",
    "\n",
    "The next part of the code only shows the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894f0c8-8f17-489a-8042-8ddd47a0a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "utility = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb620c-9c2f-4c4c-b1f5-ca6acc135692",
   "metadata": {},
   "source": [
    "5.2.1. Gaussian process \n",
    "6. The result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f447d1-2dbb-4588-8f3c-380498b45814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3ba95-3e3c-4392-86fe-4312635bc125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inally we will compare the output of the LSTM when the unseen (test) data is used as an input after different phases of the process.\n",
    "#Plot after the first epoch.\n",
    "\n",
    "plot_prediction('Predicted and Real price - after first epoch.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e37cb4b-1811-43cc-a5a5-e337eef477e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 50 epochs\n",
    "\n",
    "plot_prediction('Predicted and Real price - after first 50 epochs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0215b7-4c07-49b4-9bfc-06d43cbeeb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction('Predicted and Real price - after first 200 epochs.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962413cb-f3c2-47a3-ae8b-2a16e2202128",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction('Final result.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85d8e0-fe8d-4f51-a676-0f799d974e52",
   "metadata": {},
   "source": [
    "As a next step, I will try to take everything separately and provide some analysis on what worked and why. Why did we receive these results and is it just by coinscidence? So stay tuned.\n",
    "What is next? \n",
    "Next, I will try to create a RL environment for testing trading algorithms that decide when and how to trade. The output from the GAN will be one of the parameters in the environment.\n",
    "About me \n",
    "www.linkedin.com/in/borisbanushev\n",
    "\n",
    "Disclaimer \n",
    "       This notebook is entirely informative. None of the content presented in this notebook constitutes a recommendation that any particular security, portfolio of securities, transaction or investment strategy is suitable for any specific person. Futures, stocks and options trading involves substantial risk of loss and is not suitable for every investor. The valuation of futures, stocks and options may fluctuate, and, as a result, clients may lose more than their original investment.\n",
    "\n",
    "       All trading strategies are used at your own risk.\n",
    "\n",
    "       There are many many more details to explore - in choosing data features, in choosing algorithms, in tuning the algos, etc. This version of the notebook itself took me 2 weeks to finish. I am sure there are many unaswered parts of the process. So, any comments and suggestion - please do share. I'd be happy to add and test any ideas in the current process.\n",
    "\n",
    "Thanks for reading.\n",
    "\n",
    "Best, Boris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f905f-8081-4715-b033-5dc5ffec99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thank you, Boris"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
